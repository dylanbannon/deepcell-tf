{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a convolutional neural network\n",
    "The python script training_template.py is used to train a new convolutional neural network (conv-net). This python notebook outlines this script in a step-by-step fashion.\n",
    "\n",
    "The first step is to load the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Quadro K620 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from cnn_functions import rate_scheduler, train_model_sample\n",
    "from model_zoo import bn_feature_net_61x61 as the_model\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning model we're are choosing is loaded from the model_zoo.py file. Here we are using the bn_feature_net_61x61 function. As a rule of thumb, the receptive field of the neural network (61 x 61 pixels) should roughly match the size of the cell.\n",
    "\n",
    "We next define the batch size (the number of images the conv-net processes at once) and the number of epochs (the number of times we cycle through the entire training data set during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_epoch = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we determine which training dataset we use, record which network we use, and define the directory locations of where the training data resides and where we want our network parameters saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"nuclei_all_61x61\"\n",
    "expt = \"bn_feature_net_61x61\"\n",
    "\n",
    "direc_save = \"./trained_networks/\"\n",
    "direc_data = \"./training_data_npz/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define which optimizer we will use for training. In our experience, stochastic gradient descent works well for batch normalized networks while RMSprop appears to work better for non-batch normalized networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr = 0.1, decay = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will train the conv-net model on our training data. We typically train 5 models at a time so we can average the predictions - we've found that using model parallelism in this fashion leads to more rebust segmentation. Because our training data has 2 channels (phase image and nuclear marker) and we are looking for 3 features (edge, interior, and background), we need to specify those flags. The last piece of code is necessary to make sure that the names of each layer in the conv-net are assigned appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using feature net 61x61 with batch normalization\n",
      "('X_train shape:', (8, 1, 1080, 1280))\n",
      "(481443, 'train samples')\n",
      "(53493, 'test samples')\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/25\n",
      "   700/481443 [..............................] - ETA: 6584s - loss: 1.2568 - acc: 0.5171"
     ]
    }
   ],
   "source": [
    "for iterate in xrange(5):\n",
    "\n",
    "\tmodel = the_model(n_channels = 1, n_features = 3, reg = 1e-5)\n",
    "\n",
    "\ttrain_model_sample(model = model, dataset = dataset, optimizer = optimizer, \n",
    "\t\texpt = expt, it = iterate, batch_size = batch_size, n_epoch = n_epoch,\n",
    "\t\tdirec_save = direc_save, \n",
    "\t\tdirec_data = direc_data, \n",
    "\t\tlr_sched = lr_sched,\n",
    "\t\trotate = True, flip = True, shear = False)\n",
    "\n",
    "\tdel model\n",
    "\tfrom keras.backend.common import _UID_PREFIXES\n",
    "\tfor key in _UID_PREFIXES.keys():\n",
    "\t\t_UID_PREFIXES[key] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
