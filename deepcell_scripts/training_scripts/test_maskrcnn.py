"""
test_maskrcnn.py

Testing Mask RCNN

@author: David Van Valen
"""

import numpy as np
import os
import scipy
from deepcell import MaskRCNN, bn_dense_net_backbone, Config, Dataset, get_data
from deepcell.dc_helper_functions import *
from tensorflow.python.keras import backend as K
from skimage.measure import label, regionprops



def load_image_gt(dataset, config, image_id, augment=False,
				  use_mini_mask=False):
	"""Load and return ground truth data for an image (image, mask, bounding boxes).
	augment: If true, apply random image augmentation. Currently, only
		horizontal flipping is offered.
	use_mini_mask: If False, returns full-size masks that are the same height
		and width as the original image. These can be big, for example
		1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
		224x224 and are generated by extracting the bounding box of the
		object and resizing it to MINI_MASK_SHAPE.
	Returns:
	image: [height, width, 3]
	shape: the original shape of the image before resizing and cropping.
	class_ids: [instance_count] Integer class IDs
	bbox: [instance_count, (y1, x1, y2, x2)]
	mask: [height, width, instance_count]. The height and width are those
		of the image unless use_mini_mask is True, in which case they are
		defined in MINI_MASK_SHAPE.
	"""
	# Load image and mask
	image = dataset.load_image(image_id)
	mask, class_ids = dataset.load_mask(image_id)
	shape = image.shape
	image, window, scale, padding = resize_image(
		image,
		min_dim=config.IMAGE_MIN_DIM,
		max_dim=config.IMAGE_MAX_DIM,
		padding=config.IMAGE_PADDING)
	mask = resize_mask(mask, scale, padding)

	# Random horizontal flips.
	if augment:
		if random.randint(0, 1):
			image = np.fliplr(image)
			mask = np.fliplr(mask)

	# Bounding boxes. Note that some boxes might be all zeros
	# if the corresponding mask got cropped out.
	# bbox: [num_instances, (y1, x1, y2, x2)]
	bbox = extract_bboxes(mask)

	# Active classes
	# Different datasets have different classes, so track the
	# classes supported in the dataset of this image.
	active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)
	source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]["source"]]
	active_class_ids[source_class_ids] = 1

	# Resize masks to smaller size to reduce memory usage
	if use_mini_mask:
		mask = minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)

	# Image meta data
	image_meta = compose_image_meta(image_id, shape, window, active_class_ids)

	return image, image_meta, class_ids, bbox, mask

def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):
	"""Given the anchors and GT boxes, compute overlaps and identify positive
	anchors and deltas to refine them to match their corresponding GT boxes.
	anchors: [num_anchors, (y1, x1, y2, x2)]
	gt_class_ids: [num_gt_boxes] Integer class IDs.
	gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]
	Returns:
	rpn_match: [N] (int32) matches between anchors and GT boxes.
			   1 = positive anchor, -1 = negative anchor, 0 = neutral
	rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
	"""
	# RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
	rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
	# RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
	rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

	# Handle COCO crowds
	# A crowd box in COCO is a bounding box around several instances. Exclude
	# them from training. A crowd box is given a negative class ID.
	crowd_ix = np.where(gt_class_ids < 0)[0]
	if crowd_ix.shape[0] > 0:
		# Filter out crowds from ground truth class IDs and boxes
		non_crowd_ix = np.where(gt_class_ids > 0)[0]
		crowd_boxes = gt_boxes[crowd_ix]
		gt_class_ids = gt_class_ids[non_crowd_ix]
		gt_boxes = gt_boxes[non_crowd_ix]
		# Compute overlaps with crowd boxes [anchors, crowds]
		crowd_overlaps = compute_overlaps(anchors, crowd_boxes)
		crowd_iou_max = np.amax(crowd_overlaps, axis=1)
		no_crowd_bool = (crowd_iou_max < 0.001)
	else:
		# All anchors don't intersect a crowd
		no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

	# Compute overlaps [num_anchors, num_gt_boxes]
	overlaps = compute_overlaps(anchors, gt_boxes)

	# Match anchors to GT Boxes
	# If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
	# If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
	# Neutral anchors are those that don't match the conditions above,
	# and they don't influence the loss function.
	# However, don't keep any GT box unmatched (rare, but happens). Instead,
	# match it to the closest anchor (even if its max IoU is < 0.3).
	#
	# 1. Set negative anchors first. They get overwritten below if a GT box is
	# matched to them. Skip boxes in crowd areas.
	anchor_iou_argmax = np.argmax(overlaps, axis=1)
	anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
	rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
	# 2. Set an anchor for each GT box (regardless of IoU value).
	# TODO: If multiple anchors have the same IoU match all of them
	gt_iou_argmax = np.argmax(overlaps, axis=0)
	rpn_match[gt_iou_argmax] = 1
	# 3. Set anchors with high overlap as positive.
	rpn_match[anchor_iou_max >= 0.7] = 1

	# Subsample to balance positive and negative anchors
	# Don't let positives be more than half the anchors
	ids = np.where(rpn_match == 1)[0]
	extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
	if extra > 0:
		# Reset the extra ones to neutral
		ids = np.random.choice(ids, extra, replace=False)
		rpn_match[ids] = 0
	# Same for negative proposals
	ids = np.where(rpn_match == -1)[0]
	extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -
						np.sum(rpn_match == 1))
	if extra > 0:
		# Rest the extra ones to neutral
		ids = np.random.choice(ids, extra, replace=False)
		rpn_match[ids] = 0

	# For positive anchors, compute shift and scale needed to transform them
	# to match the corresponding GT boxes.
	ids = np.where(rpn_match == 1)[0]
	ix = 0  # index into rpn_bbox
	# TODO: use box_refinement() rather than duplicating the code here
	for i, a in zip(ids, anchors[ids]):
		# Closest gt box (it might have IoU < 0.7)
		gt = gt_boxes[anchor_iou_argmax[i]]

		# Convert coordinates to center plus width/height.
		# GT Box
		gt_h = gt[2] - gt[0]
		gt_w = gt[3] - gt[1]
		gt_center_y = gt[0] + 0.5 * gt_h
		gt_center_x = gt[1] + 0.5 * gt_w
		# Anchor
		a_h = a[2] - a[0]
		a_w = a[3] - a[1]
		a_center_y = a[0] + 0.5 * a_h
		a_center_x = a[1] + 0.5 * a_w

		# Compute the bbox refinement that the RPN should predict.
		rpn_bbox[ix] = [
			(gt_center_y - a_center_y) / a_h,
			(gt_center_x - a_center_x) / a_w,
			np.log(gt_h / a_h),
			np.log(gt_w / a_w),
		]
		# Normalize
		rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
		ix += 1

	return rpn_match, rpn_bbox




def mrcnn_data_generator(dataset, config, shuffle=True, augment=True, random_rois=0,
				   batch_size=1, detection_targets=False):
	"""A generator that returns images and corresponding target class ids,
	bounding box deltas, and masks.
	dataset: The Dataset object to pick data from
	config: The model config object
	shuffle: If True, shuffles the samples before every epoch
	augment: If True, applies image augmentation to images (currently only
			 horizontal flips are supported)
	random_rois: If > 0 then generate proposals to be used to train the
				 network classifier and mask heads. Useful if training
				 the Mask RCNN part without the RPN.
	batch_size: How many images to return in each call
	detection_targets: If True, generate detection targets (class IDs, bbox
		deltas, and masks). Typically for debugging or visualizations because
		in trainig detection targets are generated by DetectionTargetLayer.
	Returns a Python generator. Upon calling next() on it, the
	generator returns two lists, inputs and outputs. The containtes
	of the lists differs depending on the received arguments:
	inputs list:
	- images: [batch, H, W, C]
	- image_meta: [batch, size of image meta]
	- rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
	- rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
	- gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
	- gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
	- gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
				are those of the image unless use_mini_mask is True, in which
				case they are defined in MINI_MASK_SHAPE.
	outputs list: Usually empty in regular training. But if detection_targets
		is True then the outputs list contains target class_ids, bbox deltas,
		and masks.
	"""
	b = 0  # batch item index
	image_index = -1
	image_ids = np.copy(dataset.image_ids)
	error_count = 0

	# Anchors
	# [anchor_count, (y1, x1, y2, x2)]
	anchors = generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,
											 config.RPN_ANCHOR_RATIOS,
											 config.BACKBONE_SHAPES,
											 config.BACKBONE_STRIDES,
											 config.RPN_ANCHOR_STRIDE)

	# Keras requires a generator to run indefinately.
	while True:
		try:
			# Increment index to pick next image. Shuffle if at the start of an epoch.
			image_index = (image_index + 1) % len(image_ids)
			if shuffle and image_index == 0:
				np.random.shuffle(image_ids)

			# Get GT bounding boxes and masks for image.
			image_id = image_ids[image_index]
			image, image_meta, gt_class_ids, gt_boxes, gt_masks = \
				load_image_gt(dataset, config, image_id, augment=augment,
							  use_mini_mask=config.USE_MINI_MASK)

			# Skip images that have no instances. This can happen in cases
			# where we train on a subset of classes and the image doesn't
			# have any of the classes we care about.
			if not np.any(gt_class_ids > 0):
				continue

			# RPN Targets
			rpn_match, rpn_bbox = build_rpn_targets(image.shape, anchors,
													gt_class_ids, gt_boxes, config)

			# Mask R-CNN Targets
			if random_rois:
				rpn_rois = generate_random_rois(
					image.shape, random_rois, gt_class_ids, gt_boxes)
				if detection_targets:
					rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask =\
						build_detection_targets(
							rpn_rois, gt_class_ids, gt_boxes, gt_masks, config)

			# Init batch arrays
			if b == 0:
				batch_image_meta = np.zeros(
					(batch_size,) + image_meta.shape, dtype=image_meta.dtype)
				batch_rpn_match = np.zeros(
					[batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype)
				batch_rpn_bbox = np.zeros(
					[batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
				batch_images = np.zeros(
					(batch_size,) + image.shape, dtype=np.float32)
				batch_gt_class_ids = np.zeros(
					(batch_size, config.MAX_GT_INSTANCES), dtype=np.int32)
				batch_gt_boxes = np.zeros(
					(batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32)
				if config.USE_MINI_MASK:
					batch_gt_masks = np.zeros((batch_size, config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1],
											   config.MAX_GT_INSTANCES))
				else:
					batch_gt_masks = np.zeros(
						(batch_size, image.shape[0], image.shape[1], config.MAX_GT_INSTANCES))
				if random_rois:
					batch_rpn_rois = np.zeros(
						(batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)
					if detection_targets:
						batch_rois = np.zeros(
							(batch_size,) + rois.shape, dtype=rois.dtype)
						batch_mrcnn_class_ids = np.zeros(
							(batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)
						batch_mrcnn_bbox = np.zeros(
							(batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)
						batch_mrcnn_mask = np.zeros(
							(batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

			# If more instances than fits in the array, sub-sample from them.
			if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:
				ids = np.random.choice(
					np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
				gt_class_ids = gt_class_ids[ids]
				gt_boxes = gt_boxes[ids]
				gt_masks = gt_masks[:, :, ids]

			# Add to batch
			batch_image_meta[b] = image_meta
			batch_rpn_match[b] = rpn_match[:, np.newaxis]
			batch_rpn_bbox[b] = rpn_bbox
			batch_images[b] = mold_image(image.astype(np.float32), config)
			batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids
			batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes
			batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks
			if random_rois:
				batch_rpn_rois[b] = rpn_rois
				if detection_targets:
					batch_rois[b] = rois
					batch_mrcnn_class_ids[b] = mrcnn_class_ids
					batch_mrcnn_bbox[b] = mrcnn_bbox
					batch_mrcnn_mask[b] = mrcnn_mask
			b += 1

			# Batch full?
			if b >= batch_size:
				inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
						  batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]
				outputs = []

				if random_rois:
					inputs.extend([batch_rpn_rois])
					if detection_targets:
						inputs.extend([batch_rois])
						# Keras requires that output and targets have the same number of dimensions
						batch_mrcnn_class_ids = np.expand_dims(
							batch_mrcnn_class_ids, -1)
						outputs.extend(
							[batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])

				yield inputs, outputs

				# start a new batch
				b = 0
		except (GeneratorExit, KeyboardInterrupt):
			raise
		except:
			# Log it and skip the image
			logging.exception("Error processing image {}".format(
				dataset.image_info[image_id]))
			error_count += 1
			if error_count > 5:
				raise













class CellConfig(Config):
	NAME = "Cells"
	GPU_COUNT = 1
	IMAGES_PER_GPU = 1
	NUM_CLASSES = 2
	IMAGE_MIN_DIM = 512
	IMAGE_MAX_DIM = 512

class CellDataset(Dataset):
	def __init__(self, train_dict, **kwargs):

		super(CellDataset, self).__init__(**kwargs)

		self.x = np.asarray(train_dict["channels"], dtype = K.floatx())
		self.y = train_dict["labels"]

		for image_id in xrange(self.x.shape[0]):
			self.add_image(source = '', path = '', image_id = image_id)
		for feature in xrange(0,self.y.shape[1]-1):
			self.add_class(source = '', class_id = feature + 1, class_name = 'feature_' + str(feature + 1))
		
	def load_image(self, image_id):
		return self.x[image_id,:,:,:]

	def load_mask(self, image_id):
		mask = self.y[image_id,:,:,:]
		mask_list = []
		class_ids = []

		feature = 1
		label_mask = label(mask[feature,:,:])
		for cell_id in xrange(1, np.amax(label_mask)):
			mask_list.append(label_mask == cell_id)
			class_ids.append(1)

		masks = np.stack(mask_list, axis = -1)
		class_ids = np.array(class_ids)

		return masks, class_ids

cell_config = Config()
cell_config.NAME = 'dense_net'
cell_config.IMAGE_SHAPE = np.array([1, 512, 512])
cell_config.IMAGE_MAX_DIM = 512
cell_config.IMAGE_MIN_DIM = 256

direc_data = '/data/training_data_npz/nuclei/'
dataset = 'nuclei_conv_61x61'
training_data_file_name = os.path.join(direc_data, dataset + ".npz")
train_dict, val_dict = get_data(training_data_file_name, mode = 'bbox')

train_dataset = CellDataset(train_dict)
val_dataset = CellDataset(val_dict)

train_dataset.prepare()
val_dataset.prepare()

print train_dataset.image_ids
print val_dataset.image_ids

train_generator = mrcnn_data_generator(train_dataset, cell_config, shuffle=True, augment=False, random_rois=0,
				   batch_size=1, detection_targets=False)
val_generator = mrcnn_data_generator(val_dataset, cell_config, shuffle=True, augment=False, random_rois=0,
				   batch_size=1, detection_targets=False)

dense_rcnn = MaskRCNN(mode = 'training', config = cell_config, model_dir = '/data', backbone_graph = bn_dense_net_backbone)
dense_rcnn.compile(learning_rate = 0.001, momentum = 0.99)
dense_rcnn.train(train_dataset, val_dataset, 1e-3, 10, 'all')